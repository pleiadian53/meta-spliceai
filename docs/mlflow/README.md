## MLflow: Global Usage Guide

This guide covers global (project-wide) MLflow usage: starting/stopping servers, UI access, artifact locations, and running MLflow as a user service. For package-level workflow specifics, see `meta_spliceai/splice_engine/meta_models/workflows/inference/docs/MLFLOW_SETUP_GUIDE.md`.

### Environments and prerequisites

- Activate the project environment (mamba preferred):
```bash
mamba activate surveyor || conda activate surveyor
```
- Optional scripts (generated by the setup script):
  - `./start_mlflow.sh` starts a tracking server bound to `127.0.0.1:5000` using:
    - DB: `$HOME/output/mlflow_data/db/mlflow.db`
    - Artifact root: `$HOME/output/mlflow_data/artifacts`

### Option A: Simple local UI against a local file store

If you do not set a tracking URI, MLflow uses a local `./mlruns` directory relative to your current working directory.

```bash
# Start from your project root so UI points at ./mlruns there
mlflow ui -h 127.0.0.1 -p 5000

# Browse UI
# http://127.0.0.1:5000
```

Alternatively, to centralize the local file store under your home directory, point the UI at an explicit path and use the matching file tracking URI when running clients:

```bash
# Centralized file store (no tracking server; UI reads files directly)
mlflow ui -h 127.0.0.1 -p 5000 \
  --backend-store-uri "$HOME/output/mlflow_data/mlruns"

# For runs to appear here, clients must log to the same file store:
export MLFLOW_TRACKING_URI="file://$HOME/output/mlflow_data/mlruns"
```

### Option B: Central tracking server (recommended for consistency)

Use the provided setup to generate `./start_mlflow.sh` and an optional user service.

```bash
# One-time (or to update configs); safe to re-run
bash scripts/mlflow/setup_mlflow_remote.sh

# Start the server in foreground
./start_mlflow.sh

# Point clients at the server
export MLFLOW_TRACKING_URI=http://127.0.0.1:5000
```

SSH port forwarding (if remote VM):
```bash
ssh -L 5000:localhost:5000 <user>@<host>
# Then open http://localhost:5000 locally
```

### VSCode Remote-SSH: automatic port forwarding (alternative)

VSCode can auto-forward MLflow and Jupyter ports when you open this project over Remote-SSH. Create `.vscode/settings.json` in your project with:

```json
{
  "python.defaultInterpreterPath": "/home/bchiu/.local/share/mamba/envs/surveyor/bin/python",
  "remote.portsAttributes": {
    "5000": { "label": "MLflow", "onAutoForward": "openPreview" },
    "8888": { "label": "Jupyter", "onAutoForward": "openPreview" }
  },
  "remote.autoForwardPorts": true
}
```

Notes:
- Ensure your MLflow server binds to `127.0.0.1` (as in `./start_mlflow.sh`) for security.
- VSCode will detect the listening ports and open a preview automatically; you can pop it out to a browser.
- This avoids manual `ssh -L ...` commands when using Remote-SSH.

### Using MLflow from the inference workflow

Run with tracking enabled and (optionally) a custom experiment name. Choose ONE of the following modes and keep the UI and clients consistent with that mode.

#### A) Local file mode (no tracking server)

1) Start the UI reading a filesystem store directly (no REST API):
```bash
mlflow ui -h 127.0.0.1 -p 5000 \
  --backend-store-uri "$HOME/output/mlflow_data/mlruns"
```

2) Run the inference workflow and log runs to that same file store:
```bash
python -m meta_spliceai.splice_engine.meta_models.workflows.inference.main_inference_workflow \
  --model "$HOME/work/meta-spliceai/results/gene_cv_pc_1000_3mers_run_4" \
  --training-dataset "$HOME/work/meta-spliceai/train_pc_1000_3mers" \
  --genes ENSG00000284616,ENSG00000115705 \
  --complete-coverage \
  --output-dir "$HOME/work/meta-spliceai/test_pc_1000_3mers/predictions/scenario2b" \
  --inference-mode hybrid \
  --mlflow-enable \
  --mlflow-experiment surveyor-inference \
  --mlflow-tracking-uri "file://$HOME/output/mlflow_data/mlruns" \
  --verbose
```

3) View runs at `http://127.0.0.1:5000` (this UI reads the file store directly).

Notes:
- This does NOT start a tracking server; the UI is a standalone process that reads files.
- You cannot make `mlflow ui` talk to an HTTP server; it only reads a backend store directly.

#### B) Server mode (recommended)

1) Start the tracking server (provides REST API + built-in UI):
```bash
./start_mlflow.sh
```
If `./start_mlflow.sh` is not present, generate it first:
```bash
bash scripts/mlflow/setup_mlflow_remote.sh
```

2) Run the inference workflow and log to the server:
```bash
python -m meta_spliceai.splice_engine.meta_models.workflows.inference.main_inference_workflow \
  --model "$HOME/work/meta-spliceai/results/gene_cv_pc_1000_3mers_run_4" \
  --training-dataset "$HOME/work/meta-spliceai/train_pc_1000_3mers" \
  --genes ENSG00000284616,ENSG00000115705 \
  --complete-coverage \
  --output-dir "$HOME/work/meta-spliceai/test_pc_1000_3mers/predictions/scenario2b" \
  --inference-mode hybrid \
  --mlflow-enable \
  --mlflow-experiment surveyor-inference \
  --mlflow-tracking-uri http://127.0.0.1:5000 \
  --verbose
```

3) View runs at `http://127.0.0.1:5000` (served by the tracking server). Do not start a separate `mlflow ui` in this mode.

Notes:
- Experiment names are arbitrary; they are created if missing.
- The workflow logs parameters, metrics, and artifacts (log file, manifests, per‑gene outputs). Large intermediates stay only in your `--output-dir`. A directory tree snapshot is logged to help you locate big files.

Consistency checklist:
- If you used `mlflow ui`, ensure the clients use `MLFLOW_TRACKING_URI=file://...` that matches the UI’s `--backend-store-uri`.
- If you used `mlflow server`/`./start_mlflow.sh`, ensure clients use `MLFLOW_TRACKING_URI=http://127.0.0.1:5000` and browse the same URL.
- Do not mix both in the same session; a single UI shows a single backend store.

### Artifact and data locations

- With the centralized server (via `./start_mlflow.sh`):
  - DB: `$HOME/output/mlflow_data/db/mlflow.db`
  - Artifacts: `$HOME/output/mlflow_data/artifacts/<experiment>/<run_id>/...`
- With local UI (no tracking URI):
  - Local store: `./mlruns` (relative to where you run)
- Inference outputs on disk (source of truth for big files): your `--output-dir`

### Graceful shutdown procedures

Pick the method that matches how you started MLflow.

- If started in the foreground (UI or server):
  - Press Ctrl+C in that terminal.

- If started in background (nohup/another shell):
```bash
# Find the process on port 5000
lsof -ti:5000 -sTCP:LISTEN

# Graceful stop
kill <PID>

# If it does not exit after a few seconds
kill -9 <PID>

# Verify (no output means stopped)
lsof -ti:5000 -sTCP:LISTEN
```

- If started as a user service (systemd):
```bash
# Start or enable the service
systemctl --user daemon-reload   # if the unit file was just created/changed
systemctl --user enable --now mlflow

# Check status
systemctl --user status mlflow

# Graceful stop
systemctl --user stop mlflow

# Disable on boot for your user
systemctl --user disable mlflow

# View logs
journalctl --user-unit mlflow -e -f
```

About user services:
- User services run under your account without root.
- Unit files live under `~/.config/systemd/user/` (the setup script creates `mlflow.service`).
- They keep servers running in the background even if your shell exits (until you stop them), and they can be enabled to start on login.

### Common pitfalls

- UI and runs must point at the same backend store. If you launched `mlflow ui` from a different directory than your runs, you may see an empty UI. Prefer the centralized server or pass an explicit backend store to the UI.
- On remote VMs, bind to `127.0.0.1` and use SSH port forwarding for security.
- Ensure your `surveyor` environment is active (mamba/conda) before starting the server or running the workflow.

### See also

- Package-level guide with examples and options:
  - `meta_spliceai/splice_engine/meta_models/workflows/inference/docs/MLFLOW_SETUP_GUIDE.md`


