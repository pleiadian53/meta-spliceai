# Experiment: RunPods A40 - 50K SpliceVarDB Dataset

**Date**: December 17, 2025  
**Platform**: RunPods (NVIDIA A40, 47.6 GB VRAM)  
**Status**: ‚ö†Ô∏è Complete - Unexpected Results

---

## Objective

Scale up the ValidatedDeltaPredictor training from 8K to 50K samples to improve prediction accuracy. Previous experiments on local M1 Mac achieved r=0.507 with 8K samples; targeting r=0.55+ with full dataset.

## Hardware Configuration

| Component | Specification |
|-----------|---------------|
| GPU | NVIDIA A40 |
| VRAM | 47.6 GB |
| Platform | RunPods |
| Cost | ~$0.69/hr |

## Dataset

| Category | Count |
|----------|-------|
| Total variants | 50,715 |
| Splice-altering | 13,673 |
| Normal | 11,358 |
| Low-frequency | 25,601 |
| Conflicting | 83 |

### Train/Test Split
- **Train**: 49,282 variants (chromosomes 1-20, X)
- **Test**: 1,433 variants (chromosomes 21, 22)
- **Balanced training**: 22,132 samples (11,066 SA + 11,066 Normal)

---

## Experiments

### 1. Quick Test (Sanity Check)

**Config**: 1,000 samples, 40 epochs, batch_size=32

| Metric | Result |
|--------|--------|
| Pearson correlation | r = 0.504 |
| ROC-AUC | 0.583 |
| PR-AUC | 0.616 |
| Status | ‚úÖ Complete |

**Notes**: Baseline established, GPU pipeline working correctly.

---

### 2. Full Dataset (50K samples)

**Config**: 
- max_train=50,000
- max_test=5,000  
- epochs=50
- batch_size=64
- hidden_dim=128
- n_layers=6

| Metric | Expected | Result | Notes |
|--------|----------|--------|-------|
| Pearson correlation | r > 0.55 | **r = 0.353** | ‚ö†Ô∏è Lower than quick test! |
| ROC-AUC | > 0.65 | 0.582 | Similar to quick test |
| PR-AUC | > 0.70 | **0.692** | ‚úÖ Improved |
| Detection @ 0.1 | - | 28.2% | - |
| False positive | - | 14.0% | - |

**Duration**: 52.4 minutes  
**Status**: ‚ö†Ô∏è Complete - Correlation degraded

### Training Loss Progression
```
Epoch  1/50: loss = 0.014082
Epoch 10/50: loss = 0.011290
Epoch 20/50: loss = 0.010972
Epoch 30/50: loss = 0.010499
Epoch 40/50: loss = 0.005335
Epoch 50/50: loss = 0.001918  ‚Üê Model converged
```

### ‚ö†Ô∏è Analysis: Why Did Correlation Decrease?

This is counterintuitive - more data should improve results. Possible causes:

1. **Overfitting**: Loss decreased to 0.002 which may be too low. Model memorized training patterns.

2. **Test Set Difference**: Chromosomes 21/22 may have different characteristics than training set.

3. **Target Noise Amplification**: With more samples, the base model's predictions (used as targets for SA variants) may have accumulated more noise.

4. **Class Distribution Shift**: The balanced sampling may not represent the true test distribution.

5. **Hyperparameter Mismatch**: Quick test with 1K samples may need different hyperparameters than 50K.

### Recommended Next Steps

1. ‚úÖ **Add regularization**: Increase dropout, add early stopping
2. ‚úÖ **Reduce epochs**: Stop at epoch 20-30 based on validation loss
3. ‚úÖ **Use validation set**: Monitor validation loss during training
4. ‚è≥ **Analyze per-class performance**: Check SA vs Normal correlation separately
5. ‚è≥ **Learning rate schedule**: Use cosine annealing or reduce on plateau

---

### 3. Early Stopping Experiment (In Progress)

**Config**: 
- max_train=50,000 (85% train, 15% validation)
- max_epochs=100 (will stop early)
- patience=7 epochs
- batch_size=128
- hidden_dim=256
- n_layers=8

| Metric | Expected | Result |
|--------|----------|--------|
| Correlation | r > 0.50 | üîÑ Running |
| ROC-AUC | > 0.60 | üîÑ Running |
| Stopped at epoch | 15-30 | üîÑ Running |

**Start time**: 2025-12-17 05:10:50 UTC

**Key Changes**:
- 15% validation split for early stopping
- Patience=7: stop after 7 epochs without improvement
- Restores best model weights when stopped

---

### 3. HyenaDNA Experiment (Planned)

**Config**: HyenaDNA encoder with validated delta targets

| Parameter | Value |
|-----------|-------|
| Model | hyenadna-tiny-1k-seqlen |
| Freeze encoder | True (initial) |
| Epochs | 40 |

**Status**: ‚è≥ Pending (after full_dataset completes)

---

## Observations

### Data Preparation Insights

1. **SpliceVarDB Distribution**: 
   - ~27% Splice-altering (clear positive class)
   - ~22% Normal (clear negative class)  
   - ~51% Low-frequency/Conflicting (uncertain, excluded from training)

2. **Balanced Sampling**: Using equal SA/Normal samples helps prevent class imbalance issues.

3. **Validated Delta Target Strategy**:
   - SA variants: Use base model delta (trusted because SpliceVarDB confirms)
   - Normal variants: Force zero delta (override base model - we know there's no effect)
   - This creates high-quality training signal

### GPU Utilization

- A40 with 47.6 GB VRAM easily handles batch_size=64
- Could potentially increase to batch_size=128 for faster training

---

## Commands to Monitor

```bash
# Check experiment progress
tail -f /workspace/meta-spliceai/gpu_exp_full_50k.log

# Attach to tmux session
tmux attach -t gpu_exp

# Check GPU usage
nvidia-smi
```

---

## Results Summary

| Experiment | Correlation | ROC-AUC | PR-AUC | Stopped | Status |
|------------|-------------|---------|--------|---------|--------|
| Quick Test (1K) | **0.504** | 0.583 | 0.616 | 10/10 | ‚úÖ |
| Full Dataset (50K) | 0.353 | 0.582 | **0.692** | 50/50 | ‚ö†Ô∏è Overfit |
| Early Stopping | - | - | - | - | üîÑ Running |
| HyenaDNA | - | - | - | - | ‚è≥ |

**Key Finding**: More data led to *worse* correlation but better PR-AUC. This suggests the model is learning to rank samples correctly (PR-AUC) but losing calibration for continuous delta prediction (correlation).

**Hypothesis**: Early stopping should prevent overfitting and maintain better correlation.

---

## Model Checkpoints

Saved to: `/workspace/meta-spliceai/data/mane/GRCh38/openspliceai_eval/meta_layer_dev/`

| Experiment | Path |
|------------|------|
| Quick Test | `20251217_034541/checkpoints/gpu_quick_test_1000_samples.pt` |
| Full Dataset | `20251217_044334/checkpoints/gpu_full_dataset_50k_samples.pt` |

---

## Next Steps

1. ‚úÖ Complete full 50K training
2. ‚è≥ Analyze correlation by variant type
3. ‚è≥ Run HyenaDNA experiment
4. ‚è≥ Compare with longer context windows
5. ‚è≥ Consider multi-step framework

---

*This experiment log will be updated as results come in.*

