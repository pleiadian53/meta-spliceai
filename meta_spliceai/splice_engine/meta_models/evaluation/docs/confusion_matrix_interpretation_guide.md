# Confusion Matrix Interpretation Guide

## Overview

This document explains how to interpret the confusion matrices generated by MetaSpliceAI's cross-validation scripts, specifically:

- `run_gene_cv_sigmoid.py` (Gene-aware CV)
- `run_loco_cv_multiclass_scalable.py` (Chromosome-aware CV)

## Matrix Format

### Standard Convention

The confusion matrices follow the **scikit-learn convention**:
- **Rows** = **True labels** (ground truth)
- **Columns** = **Predicted labels** (model predictions)

### Code Implementation

```python
# From run_gene_cv_sigmoid.py line 859
cm = confusion_matrix(y[test_idx], pred, labels=[0, 1, 2])
print("Confusion matrix (fold", fold_idx, ")\n", 
      pd.DataFrame(cm, index=["neither", "donor", "acceptor"], 
                  columns=["neither", "donor", "acceptor"]))
```

### Label Encoding

The scripts use the following label encoding:
- `0` = "donor" (donor splice sites)
- `1` = "acceptor" (acceptor splice sites)  
- `2` = "neither" (non-splice sites)

## Prediction Methods: Argmax vs Threshold-Based

### Two Different Approaches in CV Workflow

The CV workflow uses **two different prediction approaches** for different evaluation purposes:

#### **1. Multiclass Metrics (Confusion Matrix) - ARGMAX**
```python
# Line 698: Multiclass prediction using argmax
proba = np.column_stack(proba_parts)  # shape (n,3) - multiclass probabilities
pred = proba.argmax(axis=1)  # ← ARGMAX: picks class with highest probability

# Line 858: Confusion matrix using argmax predictions
cm = confusion_matrix(y[test_idx], pred, labels=[0, 1, 2])
```

**Characteristics:**
- **No threshold needed** - picks the most likely class
- **Raw model performance** - evaluates inherent classification ability
- **Used for**: Confusion matrix, overall accuracy, macro F1 score, splice site accuracy

#### **2. Binary Metrics (Base vs Meta) - THRESHOLD-BASED**
```python
# Binary prediction: threshold-based for splice vs non-splice
y_true_bin = (y[test_idx] != 0).astype(int)  # 1=splice, 0=non-splice
y_prob_meta = proba[:, 1] + proba[:, 2]      # Combined splice probability

# Different thresholds for base vs meta models
meta_metrics = _binary_metrics(y_true_bin, y_prob_meta, thresh=args.threshold or 0.5, k=args.top_k)
base_metrics = _binary_metrics(y_true_bin, y_prob_base, thresh=args.base_thresh, k=args.top_k)
```

**Characteristics:**
- **Threshold-dependent** - applies specific probability thresholds
- **Practical performance** - evaluates performance at optimal operating points
- **Used for**: F1 scores, precision/recall, error reduction analysis

### **Key Insight: F1 Scores Don't Require Thresholds**

You're absolutely correct! **F1 scores can be calculated without thresholds** using the argmax approach:

```python
# Multiclass F1 calculation from argmax predictions (no threshold needed)
pred = proba.argmax(axis=1)  # Class predictions
macro_f1 = f1_score(y[test_idx], pred, average="macro")  # ← No threshold!

# Per-class F1 scores also available
f1_donor = f1_score(y[test_idx], pred, average=None)[0]      # Donor F1
f1_acceptor = f1_score(y[test_idx], pred, average=None)[1]   # Acceptor F1
f1_neither = f1_score(y[test_idx], pred, average=None)[2]    # Neither F1
```

### **Why Two Approaches?**

#### **Multiclass (Argmax) - For Overall Performance**
- **Purpose**: Evaluate the model's ability to distinguish between all three classes
- **Method**: `argmax` picks the most likely class for each prediction
- **Advantage**: No threshold tuning needed, evaluates raw model confidence
- **F1 Calculation**: Direct from confusion matrix counts

#### **Binary (Threshold) - For Base vs Meta Comparison**
- **Purpose**: Compare base model vs meta model performance on splice site detection
- **Method**: Apply different optimal thresholds for each model
- **Advantage**: Fair comparison using each model's optimal operating point
- **F1 Calculation**: Threshold-optimized for practical use

### **Future Enhancement: Argmax for Base vs Meta Comparison**

As you suggested, we could indeed use argmax for base vs meta comparison:

```python
# Potential argmax-based comparison (future enhancement)
base_pred = base_proba.argmax(axis=1)  # Base model argmax predictions
meta_pred = meta_proba.argmax(axis=1)  # Meta model argmax predictions

# Compare using argmax (no thresholds needed)
base_f1 = f1_score(y_true, base_pred, average="macro")
meta_f1 = f1_score(y_true, meta_pred, average="macro")
```

**Benefits of argmax approach:**
- **Simpler**: No threshold tuning required
- **Fairer**: Direct comparison of model capabilities
- **Consistent**: Same method for all evaluations
- **Robust**: Not sensitive to threshold selection

## Example Matrix

```
           neither  donor  acceptor    ← PREDICTED (columns)
neither    181633    277       272    ← TRUE = neither
donor         388   8538         2    ← TRUE = donor  
acceptor      374      6      8554    ← TRUE = acceptor
```

## Cell-by-Cell Interpretation

| True Label | Predicted Label | Count | Interpretation | Metric Type |
|------------|----------------|-------|----------------|-------------|
| **neither** | **neither** | 181,633 | ✅ **True Negatives (TN)** - Correctly identified non-splice sites | Correct |
| **neither** | **donor** | 277 | ❌ **False Positives (FP)** - Non-splice sites misclassified as donor sites | Error |
| **neither** | **acceptor** | 272 | ❌ **False Positives (FP)** - Non-splice sites misclassified as acceptor sites | Error |
| **donor** | **neither** | 388 | ❌ **False Negatives (FN)** - Donor sites missed (classified as neither) | Error |
| **donor** | **donor** | 8,538 | ✅ **True Positives (TP)** - Correctly identified donor sites | Correct |
| **donor** | **acceptor** | 2 | ❌ **False Positives (FP)** - Donor sites misclassified as acceptor sites | Error |
| **acceptor** | **neither** | 374 | ❌ **False Negatives (FN)** - Acceptor sites missed (classified as neither) | Error |
| **acceptor** | **donor** | 6 | ❌ **False Positives (FP)** - Acceptor sites misclassified as donor sites | Error |
| **acceptor** | **acceptor** | 8,554 | ✅ **True Positives (TP)** - Correctly identified acceptor sites | Correct |

## Performance Metrics Calculation

### Per-Class Metrics

#### Donor Sites (Class 0)
- **True Positives (TP)**: 8,538
- **False Positives (FP)**: 277 + 2 = 279
- **False Negatives (FN)**: 388 + 6 = 394
- **True Negatives (TN)**: 181,633 + 8,554 = 190,187

**Precision** = TP / (TP + FP) = 8,538 / (8,538 + 279) = 0.968
**Recall** = TP / (TP + FN) = 8,538 / (8,538 + 394) = 0.956
**F1-Score** = 2 × (Precision × Recall) / (Precision + Recall) = 0.962

#### Acceptor Sites (Class 1)
- **True Positives (TP)**: 8,554
- **False Positives (FP)**: 272 + 6 = 278
- **False Negatives (FN)**: 374 + 2 = 376
- **True Negatives (TN)**: 181,633 + 8,538 = 190,171

**Precision** = TP / (TP + FP) = 8,554 / (8,554 + 278) = 0.969
**Recall** = TP / (TP + FN) = 8,554 / (8,554 + 376) = 0.958
**F1-Score** = 2 × (Precision × Recall) / (Precision + Recall) = 0.963

#### Non-Splice Sites (Class 2 - "neither")
- **True Negatives (TN)**: 181,633
- **False Positives (FP)**: 388 + 374 = 762
- **False Negatives (FN)**: 277 + 272 = 549
- **True Positives (TP)**: 8,538 + 8,554 = 17,092

**Precision** = TN / (TN + FP) = 181,633 / (181,633 + 762) = 0.996
**Recall** = TN / (TN + FN) = 181,633 / (181,633 + 549) = 0.997
**F1-Score** = 2 × (Precision × Recall) / (Precision + Recall) = 0.996

### Overall Metrics

#### Macro-Averaged F1-Score
```
Macro F1 = (F1_donor + F1_acceptor + F1_neither) / 3
         = (0.962 + 0.963 + 0.996) / 3
         = 0.974
```

#### Overall Accuracy
```
Accuracy = (TP_donor + TP_acceptor + TN_neither) / Total
         = (8,538 + 8,554 + 181,633) / (181,633 + 277 + 272 + 388 + 8,538 + 2 + 374 + 6 + 8,554)
         = 198,725 / 200,044
         = 0.993
```

## Key Insights from the Example

### 1. **High Overall Performance**
- Overall accuracy: 99.3%
- Excellent performance across all classes

### 2. **Class Imbalance Handling**
- The dataset is heavily imbalanced (181,633 "neither" vs ~8,500 each for donor/acceptor)
- The model handles this imbalance well, with high precision and recall for minority classes

### 3. **Error Patterns**
- **Most common error**: Missing splice sites (FN) rather than false predictions (FP)
- **Donor → Acceptor errors**: Very rare (only 2 cases)
- **Acceptor → Donor errors**: Very rare (only 6 cases)
- **Non-splice → Splice errors**: More common (277 + 272 = 549 total)

### 4. **Splice Site Detection**
- **Donor sites**: 95.6% recall (388 missed out of 8,926 total)
- **Acceptor sites**: 95.8% recall (374 missed out of 8,928 total)
- **Cross-classification**: Extremely rare (8 total cases)

## Memory Aids

### Row vs Column
- **"Row is Real"** - Rows show the true/real labels
- **"Column is Computed"** - Columns show the computed/predicted labels

### Error Types
- **False Positive (FP)**: Predicted positive when actually negative
- **False Negative (FN)**: Predicted negative when actually positive
- **True Positive (TP)**: Correctly predicted positive
- **True Negative (TN)**: Correctly predicted negative

### Quick Validation
- **Row sums** = Total instances of each true class
- **Column sums** = Total predictions of each class
- **Diagonal sum** = Total correct predictions
- **Off-diagonal sum** = Total incorrect predictions

## Common Questions

### Q: What does the 374 in the acceptor row mean?
**A**: 374 true acceptor sites were incorrectly classified as "neither" (non-splice sites). This is a **False Negative** for acceptor detection.

### Q: What does the 277 in the neither row mean?
**A**: 277 non-splice sites were incorrectly classified as donor sites. This is a **False Positive** for donor detection.

### Q: Why are there zeros in some cells?
**A**: Zeros indicate that the model never made that specific type of error. For example, if there are 0 donor→acceptor errors, it means the model never confused donor sites for acceptor sites.

### Q: How do I know if the model is performing well?
**A**: Look for:
- High numbers on the diagonal (correct predictions)
- Low numbers off the diagonal (errors)
- Balanced performance across all classes
- High overall accuracy (>95% for this domain)

### Q: Can F1 scores be calculated without thresholds?
**A**: **Yes!** F1 scores can be calculated directly from argmax predictions using the confusion matrix. The formula is:
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```
Where Precision and Recall are calculated from the confusion matrix counts (TP, FP, FN, TN).

### Q: Why use threshold-based comparison for base vs meta models?
**A**: Threshold-based comparison allows each model to operate at its optimal performance point. However, argmax comparison would be simpler and potentially fairer for direct model capability comparison.

## Troubleshooting

### Poor Performance Indicators
1. **Low diagonal values**: Model is making many incorrect predictions
2. **High off-diagonal values**: Model is confusing classes frequently
3. **Unbalanced errors**: Model favors one class over others
4. **High cross-classification errors**: Model cannot distinguish between donor and acceptor sites

### Good Performance Indicators
1. **High diagonal values**: Most predictions are correct
2. **Low off-diagonal values**: Few classification errors
3. **Balanced performance**: Similar precision/recall across classes
4. **Low cross-classification**: Model clearly distinguishes between donor and acceptor sites

## Related Documentation

- [Evaluation Modules Overview](evaluation_modules_overview.md)
- [CV Metrics Visualization Guide](../cv_metrics_viz.py)
- [Performance Analysis Tools](../baseline_error_calculator.py)

## References

- [Scikit-learn Confusion Matrix Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)
- [Understanding Confusion Matrices](https://en.wikipedia.org/wiki/Confusion_matrix)
- [Multi-class Classification Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification) 