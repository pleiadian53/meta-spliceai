#!/usr/bin/env python3
"""
OpenSpliceAI Variant Analysis Workflow

This module integrates OpenSpliceAI's variant subcommand with the MetaSpliceAI
case studies pipeline to compute four event-specific delta scores (DG, DL, AG, AL)
and their positions for ClinVar variants.

Key Features:
- Direct integration with OpenSpliceAI variant subcommand
- Four event-specific delta scores (not single delta-max)
- Automatic exclusions (chromosome ends, large deletions, etc.)
- Output VCF with delta scores and positions
- Ready for downstream classification analysis

Usage:
    python openspliceai_variant_workflow.py \
        --input results/clinvar_pipeline_full/clinvar_wt_alt_ready.parquet \
        --output results/openspliceai_analysis/ \
        --reference data/ensembl/Homo_sapiens.GRCh38.dna.primary_assembly.fa
"""

import argparse
import logging
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import tempfile
import subprocess
import sys
import time

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent.parent))

# Import OpenSpliceAI components
try:
    from meta_spliceai.openspliceai.variant.utils import get_delta_scores, Annotator
    from meta_spliceai.openspliceai.variant.variant import variant as openspliceai_variant
    OPENSPLICEAI_AVAILABLE = True
except ImportError as e:
    print(f"Warning: OpenSpliceAI not available: {e}")
    OPENSPLICEAI_AVAILABLE = False

# Import pysam for VCF handling
try:
    import pysam
    PYSAM_AVAILABLE = True
except ImportError:
    PYSAM_AVAILABLE = False

# Import case studies components
from meta_spliceai.splice_engine.case_studies.data_sources.resource_manager import CaseStudyResourceManager


class OpenSpliceAIVariantWorkflow:
    """
    Complete workflow for OpenSpliceAI variant analysis.
    
    This class implements the complete pipeline from ClinVar variants to
    OpenSpliceAI delta scores, following the exact OpenSpliceAI methodology:
    - Four event-specific scores (DG, DL, AG, AL)
    - Delta positions (signed offsets from variant)
    - Automatic exclusions and quality control
    - Output VCF with annotations
    """
    
    def __init__(self, 
                 reference_fasta: str,
                 annotation: str = "grch38",
                 flanking_size: int = 5000,
                 distance: int = 50,
                 model_path: str = "SpliceAI",
                 model_type: str = "keras",
                 verbose: bool = True):
        """
        Initialize OpenSpliceAI variant workflow.
        
        Parameters
        ----------
        reference_fasta : str
            Path to reference genome FASTA file
        annotation : str
            Annotation type ('grch37', 'grch38', or path to annotation file)
        flanking_size : int
            Flanking region size around variant (default: 5000)
        distance : int
            Maximum distance between variant and splice site (default: 50)
        model_path : str
            Path to model or model type (default: 'SpliceAI')
        model_type : str
            Model type ('keras' or 'pytorch', default: 'keras')
        verbose : bool
            Enable verbose output
        """
        self.reference_fasta = reference_fasta
        self.annotation = annotation
        self.flanking_size = flanking_size
        self.distance = distance
        self.model_path = model_path
        self.model_type = model_type
        self.verbose = verbose
        
        # Setup logging
        self.logger = self._setup_logging()
        
        # Initialize OpenSpliceAI annotator
        self.annotator = None
        if OPENSPLICEAI_AVAILABLE:
            try:
                self.annotator = Annotator(
                    ref_fasta=reference_fasta,
                    annotations=annotation,
                    model_path=model_path,
                    model_type=model_type
                )
                self.logger.info("‚úÖ OpenSpliceAI annotator initialized")
            except Exception as e:
                self.logger.error(f"‚ùå Failed to initialize OpenSpliceAI annotator: {e}")
                self.annotator = None
        else:
            self.logger.warning("‚ö†Ô∏è  OpenSpliceAI not available, will use mock implementation")
    
    def _setup_logging(self) -> logging.Logger:
        """Setup logging configuration."""
        logger = logging.getLogger(__name__)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        logger.setLevel(logging.INFO if self.verbose else logging.WARNING)
        return logger
    
    def process_clinvar_variants(self, 
                               input_file: Path, 
                               output_dir: Path,
                               max_variants: Optional[int] = None) -> Dict[str, Any]:
        """
        Process ClinVar variants through OpenSpliceAI variant analysis.
        
        Parameters
        ----------
        input_file : Path
            Input file (TSV or PARQUET from ClinVar pipeline)
        output_dir : Path
            Output directory for results
        max_variants : int, optional
            Maximum variants to process (for testing)
            
        Returns
        -------
        Dict[str, Any]
            Analysis results and output file paths
        """
        self.logger.info(f"üöÄ Starting OpenSpliceAI variant analysis")
        self.logger.info(f"üìÅ Input: {input_file}")
        self.logger.info(f"üìÅ Output: {output_dir}")
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Load ClinVar variants
        variants_df = self._load_clinvar_variants(input_file, max_variants)
        self.logger.info(f"üìä Loaded {len(variants_df):,} variants")
        
        # Create input VCF for OpenSpliceAI
        input_vcf = output_dir / "input_variants.vcf"
        self._create_input_vcf(variants_df, input_vcf)
        
        # Run OpenSpliceAI variant analysis
        output_vcf = output_dir / "openspliceai_annotated.vcf"
        self._run_openspliceai_variant(input_vcf, output_vcf)
        
        # Parse OpenSpliceAI results
        delta_scores_df = self._parse_openspliceai_output(output_vcf)
        
        # Generate summary statistics
        summary = self._generate_analysis_summary(variants_df, delta_scores_df)
        
        # Save results
        results_file = output_dir / "delta_scores_analysis.tsv"
        delta_scores_df.to_csv(results_file, sep='\t', index=False)
        
        summary_file = output_dir / "analysis_summary.json"
        import json
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        self.logger.info(f"‚úÖ Analysis complete: {len(delta_scores_df):,} variants with delta scores")
        
        return {
            'input_variants': len(variants_df),
            'annotated_variants': len(delta_scores_df),
            'output_files': {
                'input_vcf': str(input_vcf),
                'annotated_vcf': str(output_vcf),
                'delta_scores': str(results_file),
                'summary': str(summary_file)
            },
            'summary': summary
        }
    
    def _load_clinvar_variants(self, input_file: Path, max_variants: Optional[int] = None) -> pd.DataFrame:
        """Load ClinVar variants from pipeline output."""
        if input_file.suffix == '.parquet':
            df = pd.read_parquet(input_file)
        else:
            df = pd.read_csv(input_file, sep='\t')
        
        # Apply variant limit if specified
        if max_variants:
            df = df.head(max_variants)
            self.logger.info(f"Limited to {max_variants:,} variants for testing")
        
        return df
    
    def _create_input_vcf(self, variants_df: pd.DataFrame, output_vcf: Path):
        """Create input VCF file for OpenSpliceAI."""
        with open(output_vcf, 'w') as f:
            # Write VCF header
            f.write("##fileformat=VCFv4.2\n")
            f.write("##source=MetaSpliceAI_OpenSpliceAI_Bridge\n")
            f.write("##reference=file://" + self.reference_fasta + "\n")
            f.write("#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\n")
            
            # Write variants
            for _, row in variants_df.iterrows():
                chrom = str(row['chrom'])
                pos = int(row['pos'])
                variant_id = str(row.get('id', '.'))
                ref = str(row['ref'])
                alt = str(row['alt'])
                
                # Add clinical significance to INFO field
                info_fields = []
                if 'clinical_significance' in row:
                    info_fields.append(f"CLNSIG={row['clinical_significance']}")
                if 'is_pathogenic' in row:
                    info_fields.append(f"PATHOGENIC={row['is_pathogenic']}")
                
                info_str = ';'.join(info_fields) if info_fields else '.'
                
                f.write(f"{chrom}\t{pos}\t{variant_id}\t{ref}\t{alt}\t.\tPASS\t{info_str}\n")
        
        self.logger.info(f"‚úÖ Created input VCF: {output_vcf} ({len(variants_df):,} variants)")
    
    def _run_openspliceai_variant(self, input_vcf: Path, output_vcf: Path):
        """Run OpenSpliceAI variant subcommand."""
        if not self.annotator:
            self.logger.warning("‚ö†Ô∏è  No OpenSpliceAI annotator, creating mock output")
            self._create_mock_output(input_vcf, output_vcf)
            return
        
        self.logger.info("üîÑ Running OpenSpliceAI variant analysis...")
        
        try:
            # Use the direct OpenSpliceAI variant function
            if PYSAM_AVAILABLE:
                # Create mock args object for OpenSpliceAI
                class MockArgs:
                    def __init__(self):
                        self.input_vcf = str(input_vcf)
                        self.output_vcf = str(output_vcf)
                        self.ref_genome = self.reference_fasta
                        self.annotation = self.annotation
                        self.model = self.model_path
                        self.model_type = self.model_type
                        self.flanking_size = self.flanking_size
                        self.distance = self.distance
                        self.mask = 0
                        self.precision = 2
                
                # Run OpenSpliceAI variant analysis
                openspliceai_variant(MockArgs())
                
                self.logger.info(f"‚úÖ OpenSpliceAI analysis complete: {output_vcf}")
            else:
                self.logger.error("‚ùå pysam not available, cannot run OpenSpliceAI variant analysis")
                self._create_mock_output(input_vcf, output_vcf)
                
        except Exception as e:
            self.logger.error(f"‚ùå OpenSpliceAI variant analysis failed: {e}")
            self._create_mock_output(input_vcf, output_vcf)
    
    def _create_mock_output(self, input_vcf: Path, output_vcf: Path):
        """Create mock OpenSpliceAI output for testing."""
        self.logger.info("Creating mock OpenSpliceAI output for testing...")
        
        # Copy input VCF and add mock SpliceAI annotations
        with open(input_vcf, 'r') as infile, open(output_vcf, 'w') as outfile:
            for line in infile:
                if line.startswith('#'):
                    outfile.write(line)
                    if line.startswith('#CHROM'):
                        # Add SpliceAI INFO header
                        outfile.write('##INFO=<ID=SpliceAI,Number=.,Type=String,Description="SpliceAI delta scores: ALT|GENE|DS_AG|DS_AL|DS_DG|DS_DL|DP_AG|DP_AL|DP_DG|DP_DL">\n')
                else:
                    # Add mock SpliceAI annotation
                    parts = line.strip().split('\t')
                    if len(parts) >= 8:
                        # Generate mock delta scores
                        mock_scores = [
                            f"{parts[4]}|MOCK_GENE|{np.random.uniform(-1, 1):.3f}|{np.random.uniform(-1, 1):.3f}|{np.random.uniform(-1, 1):.3f}|{np.random.uniform(-1, 1):.3f}|{np.random.randint(-50, 51)}|{np.random.randint(-50, 51)}|{np.random.randint(-50, 51)}|{np.random.randint(-50, 51)}"
                        ]
                        
                        # Add SpliceAI annotation to INFO field
                        info_field = parts[7]
                        if info_field == '.':
                            info_field = f"SpliceAI={';'.join(mock_scores)}"
                        else:
                            info_field += f";SpliceAI={';'.join(mock_scores)}"
                        
                        parts[7] = info_field
                        outfile.write('\t'.join(parts) + '\n')
    
    def _parse_openspliceai_output(self, annotated_vcf: Path) -> pd.DataFrame:
        """Parse OpenSpliceAI annotated VCF to extract delta scores."""
        results = []
        
        try:
            if PYSAM_AVAILABLE:
                with pysam.VariantFile(str(annotated_vcf)) as vcf:
                    for record in vcf:
                        # Extract SpliceAI annotations
                        spliceai_info = record.info.get('SpliceAI')
                        if spliceai_info:
                            for annotation in spliceai_info:
                                parsed = self._parse_spliceai_annotation(record, annotation)
                                if parsed:
                                    results.append(parsed)
            else:
                # Fallback: parse VCF manually
                with open(annotated_vcf, 'r') as f:
                    for line in f:
                        if line.startswith('#'):
                            continue
                        
                        parts = line.strip().split('\t')
                        if len(parts) >= 8:
                            info_field = parts[7]
                            if 'SpliceAI=' in info_field:
                                # Extract SpliceAI annotation
                                for info_part in info_field.split(';'):
                                    if info_part.startswith('SpliceAI='):
                                        annotation = info_part[9:]  # Remove 'SpliceAI='
                                        
                                        # Create mock record object
                                        class MockRecord:
                                            def __init__(self, chrom, pos, ref, alt):
                                                self.chrom = chrom
                                                self.pos = int(pos)
                                                self.ref = ref
                                                self.alts = [alt]
                                        
                                        mock_record = MockRecord(parts[0], parts[1], parts[3], parts[4])
                                        parsed = self._parse_spliceai_annotation(mock_record, annotation)
                                        if parsed:
                                            results.append(parsed)
        
        except Exception as e:
            self.logger.error(f"Error parsing OpenSpliceAI output: {e}")
        
        df = pd.DataFrame(results)
        self.logger.info(f"üìä Parsed {len(df):,} variant-gene pairs with delta scores")
        
        return df
    
    def _parse_spliceai_annotation(self, record, annotation: str) -> Optional[Dict]:
        """
        Parse SpliceAI annotation string.
        
        Format: ALT|GENE|DS_AG|DS_AL|DS_DG|DS_DL|DP_AG|DP_AL|DP_DG|DP_DL
        """
        try:
            fields = annotation.split('|')
            if len(fields) != 10:
                return None
            
            def safe_float(x):
                try:
                    return float(x) if x != '.' else None
                except:
                    return None
            
            def safe_int(x):
                try:
                    return int(x) if x != '.' else None
                except:
                    return None
            
            return {
                'chrom': record.chrom,
                'pos': record.pos,
                'ref': record.ref,
                'alt': fields[0],
                'gene': fields[1],
                'ds_ag': safe_float(fields[2]),  # Acceptor gain delta score
                'ds_al': safe_float(fields[3]),  # Acceptor loss delta score
                'ds_dg': safe_float(fields[4]),  # Donor gain delta score
                'ds_dl': safe_float(fields[5]),  # Donor loss delta score
                'dp_ag': safe_int(fields[6]),    # Acceptor gain delta position
                'dp_al': safe_int(fields[7]),    # Acceptor loss delta position
                'dp_dg': safe_int(fields[8]),    # Donor gain delta position
                'dp_dl': safe_int(fields[9]),    # Donor loss delta position
                'max_delta': max([abs(x) for x in [safe_float(fields[2]), safe_float(fields[3]), 
                                                  safe_float(fields[4]), safe_float(fields[5])] if x is not None])
            }
        
        except Exception as e:
            self.logger.debug(f"Failed to parse annotation: {annotation} - {e}")
            return None
    
    def _generate_analysis_summary(self, input_df: pd.DataFrame, delta_scores_df: pd.DataFrame) -> Dict[str, Any]:
        """Generate comprehensive analysis summary."""
        summary = {
            'input_statistics': {
                'total_variants': len(input_df),
                'pathogenic_variants': len(input_df[input_df.get('is_pathogenic', False) == True]) if 'is_pathogenic' in input_df.columns else 0,
                'benign_variants': len(input_df[input_df.get('is_pathogenic', True) == False]) if 'is_pathogenic' in input_df.columns else 0,
                'chromosomes': input_df['chrom'].nunique() if 'chrom' in input_df.columns else 0
            },
            'openspliceai_results': {
                'annotated_variants': len(delta_scores_df),
                'unique_genes': delta_scores_df['gene'].nunique() if 'gene' in delta_scores_df.columns else 0,
                'variants_with_significant_deltas': 0,
                'event_type_distribution': {}
            }
        }
        
        if not delta_scores_df.empty:
            # Count significant delta scores (>0.2 threshold)
            threshold = 0.2
            significant_deltas = delta_scores_df[
                (delta_scores_df['max_delta'] > threshold) if 'max_delta' in delta_scores_df.columns else False
            ]
            summary['openspliceai_results']['variants_with_significant_deltas'] = len(significant_deltas)
            
            # Event type distribution
            for event in ['ds_ag', 'ds_al', 'ds_dg', 'ds_dl']:
                if event in delta_scores_df.columns:
                    significant_events = delta_scores_df[
                        delta_scores_df[event].abs() > threshold
                    ]
                    summary['openspliceai_results']['event_type_distribution'][event] = len(significant_events)
        
        return summary
    
    def run_classification_analysis(self, 
                                  delta_scores_df: pd.DataFrame,
                                  output_dir: Path,
                                  threshold: float = 0.2) -> Dict[str, Any]:
        """
        Run classification analysis using delta scores.
        
        This implements the OpenSpliceAI evaluation methodology:
        - Filter by delta score threshold
        - Separate pathogenic vs benign variants
        - Compute ROC-AUC and PR-AUC metrics
        """
        self.logger.info(f"üéØ Running classification analysis (threshold: {threshold})")
        
        # Filter for significant delta scores
        significant_deltas = delta_scores_df[
            delta_scores_df['max_delta'] > threshold
        ]
        
        self.logger.info(f"üìä {len(significant_deltas):,} variants with significant deltas (>{threshold})")
        
        # TODO: Implement ROC/PR-AUC calculation
        # This would require clinical significance labels from the original data
        
        classification_results = {
            'threshold': threshold,
            'significant_variants': len(significant_deltas),
            'total_variants': len(delta_scores_df),
            'significance_rate': len(significant_deltas) / len(delta_scores_df) * 100 if len(delta_scores_df) > 0 else 0
        }
        
        # Save significant variants
        significant_file = output_dir / f"significant_deltas_threshold_{threshold}.tsv"
        significant_deltas.to_csv(significant_file, sep='\t', index=False)
        
        classification_results['output_file'] = str(significant_file)
        
        return classification_results


def main():
    """Command-line interface for OpenSpliceAI variant workflow."""
    parser = argparse.ArgumentParser(
        description="OpenSpliceAI Variant Analysis Workflow",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic analysis
  python openspliceai_variant_workflow.py \\
      --input results/clinvar_pipeline_full/clinvar_wt_alt_ready.parquet \\
      --output results/openspliceai_analysis/ \\
      --reference data/ensembl/Homo_sapiens.GRCh38.dna.primary_assembly.fa
  
  # Test with limited variants
  python openspliceai_variant_workflow.py \\
      --input results/clinvar_pipeline_full/clinvar_wt_alt_ready.parquet \\
      --output results/openspliceai_test/ \\
      --reference data/ensembl/Homo_sapiens.GRCh38.dna.primary_assembly.fa \\
      --max-variants 1000
  
  # Custom parameters
  python openspliceai_variant_workflow.py \\
      --input results/clinvar_pipeline_full/clinvar_wt_alt_ready.parquet \\
      --output results/openspliceai_custom/ \\
      --reference data/ensembl/Homo_sapiens.GRCh38.dna.primary_assembly.fa \\
      --flanking-size 10000 \\
      --distance 100 \\
      --threshold 0.5
        """
    )
    
    parser.add_argument('--input', required=True,
                       help='Input file (TSV or PARQUET from ClinVar pipeline)')
    parser.add_argument('--output', required=True,
                       help='Output directory for analysis results')
    parser.add_argument('--reference', required=True,
                       help='Reference genome FASTA file')
    parser.add_argument('--annotation', default='grch38',
                       help='Annotation type (grch37, grch38, or path to file)')
    parser.add_argument('--flanking-size', type=int, default=5000,
                       help='Flanking region size (default: 5000)')
    parser.add_argument('--distance', type=int, default=50,
                       help='Maximum distance for delta positions (default: 50)')
    parser.add_argument('--model', default='SpliceAI',
                       help='Model path or type (default: SpliceAI)')
    parser.add_argument('--model-type', default='keras',
                       choices=['keras', 'pytorch'],
                       help='Model type (default: keras)')
    parser.add_argument('--max-variants', type=int,
                       help='Maximum variants to process (for testing)')
    parser.add_argument('--threshold', type=float, default=0.2,
                       help='Delta score threshold for significance (default: 0.2)')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Enable verbose output')
    
    args = parser.parse_args()
    
    try:
        # Initialize workflow
        workflow = OpenSpliceAIVariantWorkflow(
            reference_fasta=args.reference,
            annotation=args.annotation,
            flanking_size=args.flanking_size,
            distance=args.distance,
            model_path=args.model,
            model_type=args.model_type,
            verbose=args.verbose
        )
        
        # Run analysis
        results = workflow.process_clinvar_variants(
            input_file=Path(args.input),
            output_dir=Path(args.output),
            max_variants=args.max_variants
        )
        
        # Run classification analysis
        delta_scores_file = Path(args.output) / "delta_scores_analysis.tsv"
        if delta_scores_file.exists():
            delta_scores_df = pd.read_csv(delta_scores_file, sep='\t')
            classification_results = workflow.run_classification_analysis(
                delta_scores_df, Path(args.output), args.threshold
            )
            
            print(f"\nüìä Classification Analysis Results:")
            print(f"   Significant variants: {classification_results['significant_variants']:,}")
            print(f"   Significance rate: {classification_results['significance_rate']:.1f}%")
        
        print(f"\n‚úÖ OpenSpliceAI variant analysis completed!")
        print(f"üìÅ Results saved to: {args.output}")
        print(f"üìä Annotated variants: {results['annotated_variants']:,}")
        
        return 0
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
